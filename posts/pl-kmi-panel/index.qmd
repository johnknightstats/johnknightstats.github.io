---
title: "Are referees judged harshly with respect to VAR?"
image: var.jpg
author: "John Knight"
date: 2026-01-13
date-format: "D MMMM YYYY"
description: "Examining the futility of the Premier League's Key Match Incident Panel using Bayesian probability."
format:
  html:
    title-block-categories: true
categories: ["football", "statistics", "Premier League"]
execute: 
  echo: false
  eval: true
  message: false
  warning: false
engine: knitr
editor: source
draft: true

---

There are three different camps when it comes to VAR in football. Those, like me, who have been against VAR from day one: I think it ruins the excitement of goals and creates unrealistic expectations of forensic-level accuracy over every decision. There are also those who think VAR is fantastic and are very happy with it. I mean, I've never met those people, but they must exist. And the third camp consists of those who think VAR is a good idea, if only those pesky humans operating it weren't so incompetent!

The problem with that last group is, I'm not sure people are being realistic in terms of how easy it is to get refereeing decisions right. 

This [BBC article](https://www.bbc.com/sport/football/articles/cvgrx8ml7m0o) describes the increase in VAR errors in the first half of the current Premier League season, and lists the 13 VAR "mistakes", 11 of which were missed interventions and two of which were incorrect interventions.

After giving details of each incident, the article also shows a running total of which teams have suffered from or gained from these errors, just to pour a bit more petrol on the bonfire of those who think the world is conspiring against their team. Football supporters have never been renowned for their objectivity.

The mistakes are identified by the Key Match Incidents (KMI) Panel which meets every week to adjudicate on all key decisions in Premier League matches. The panel consists of one representative each from the Premier League and Professional Game Match Officials Limited (PGMOL), as well as three from a [rotating pool of former players and managers](https://www.nytimes.com/athletic/5332282/2024/03/12/referee-complaints-premier-league/) including the likes of Karen Carney, Rob Green, Terry Burton, Jonathan Walters and Steven Reid. 

We will gloss over the suitability of those individuals for the panel - judging by the general standard of punditry and co-commentary on TV, the bar is not high - but even if we respect the judgement of the panel, what are the statistics behind these mistakes actually showing us? 

According to the BBC article, here is the breakdown of the panel's votes on the 13 VAR errors:

```{r}
library(dplyr)
library(tibble)
library(gt)

panel_votes <- tribble(
  ~Panel, ~Frequency,
  "5–0",  3,
  "4–1",  3,
  "3–2",  7
) |>
  mutate(
    Percent = Frequency / sum(Frequency) * 100
  )

panel_votes |>
  gt() |>
  fmt_number(
    columns = Percent,
    decimals = 1
  ) |>
  cols_label(
    Panel = "Panel split",
    Frequency = "Frequency",
    Percent = "%"
  ) |>
  cols_align(
    align = "center",
    columns = everything()
  )
```
<p style="font-size:0.9em; color:#666; margin-top:-0.5em; text-align:center;">
  <em>More than half the ‘mistakes’ were only decided by a panel 3–2 verdict.</em>
</p>


Evidently, of the 13 decisions branded as mistakes, only three of them were unanimous 5-0 decisions by the KMI Panel. Three of them had one dissenter in a 4-1 decision and in more than half of the so-called mistakes, the panel was split 3-2! So can we really say that the referees got it wrong? What if the panel got it wrong?

## A Bayesian approach

The language surrounding refereeing decisions in football is often quite binary and hyperbolic. You will often hear "that's a stonewall penalty", or "never a pen in a million years"; sometimes about the same decision!

The fact that the panel of experts failed to unanimously agree on most of the cases shows that reality is not so clear-cut. It is reasonable to consider each refereeing decision not as a simple case of right or wrong, but instead as having a probability somewhere between 0 and 1 of being incorrect.

Under that assumption, we can model the KMI Panel's voting split as a beta-binomial distribution, where the underlying probability of the decision being incorrect follows a beta distribution, and then our five expert panelists are treated as independent Bernoulli draws and may vote correct or incorrect based on that underlying probability.

Unfortunately we don't have the full data relating to the KMI Panel's votes on every case, only on the mistakes, so we can't derive the full distribution. But what we can do is use a few different example priors, and show what our posterior probability would be of the referee's decision being incorrect after a 3-2, 4-1, or 5-0 vote.

```{r}
library(dplyr)
library(tidyr)
library(ggplot2)
library(scales)

# Your preferred palette
my_palette <- c("#233D4D", "#FF9F1C", "#41EAD4", "#FDFFFC", "#F71735")

# Distributions to compare
dists <- tibble(
  Distribution = c("Beta(1.0, 1.0)", "Beta(0.3, 2.7)", "Beta(0.05, 0.45)"),
  alpha = c(1, 0.3, 0.05),
  beta  = c(1, 2.7, 0.45)
)

dists <- dists |>
  mutate(
    Distribution = factor(
      Distribution,
      levels = c("Beta(1.0, 1.0)", "Beta(0.3, 2.7)", "Beta(0.05, 0.45)")
    )
  )


# Evenly spaced bins (width = 0.1)
breaks <- seq(0, 1, by = 0.1)
bin_labels <- sprintf("%.1f–%.1f", breaks[-length(breaks)], breaks[-1])

bin_df <- tibble(
  bin = factor(bin_labels, levels = bin_labels),
  lo = breaks[-length(breaks)],
  hi = breaks[-1]
)

# Compute bin probabilities: P(lo < q <= hi)
plot_df <- dists |>
  crossing(bin_df) |>
  mutate(
    prob = pbeta(hi, alpha, beta) - pbeta(lo, alpha, beta)
  )

ggplot(plot_df, aes(x = bin, y = prob, fill = Distribution)) +
  geom_col(width = 0.85) +
  facet_wrap(~ Distribution, nrow = 1) +
  scale_fill_manual(values = c(my_palette[1], my_palette[2], my_palette[5])) +
  scale_y_continuous(labels = percent_format(accuracy = 1)) +
  scale_x_discrete(
    breaks = c(bin_labels[1], bin_labels[length(bin_labels)]),
    labels = c("0", "1")
  ) +
  labs(
    x = "Probability of incorrect decision",
    y = "Probability mass"
  ) +
  theme_minimal(base_size = 13) +
  theme(
    legend.position = "none",
    strip.text = element_text(face = "bold"),
    panel.grid.minor = element_blank(),
    axis.title.x = element_text(size=11)
  )




```


The obvious starting point for the prior is the uniform beta (1,1). This is a flat line: we simply don't know anything about the probability of the referee's decision being correct or incorrect. But this also implies a 50% overall probability of the referee's decision being wrong, which is too high; we know from the article that there were, at minimum, 49 VAR interventions, plus other cases the panel considered while only finding 13 to be mistakes.

The second and third beta distributions I have chosen assume a mean of 0.1 probability that the referee's decision was incorrect. They vary in shape: the second distribution is a downward curve, while the third distribution is U-shaped, reflecting a larger frequency near 0 and 1: your *stonewall* and *never-in-a-million-years* decisions.

Based on these three prior distributions we can calculate the posterior probability of the ref's decision being wrong, conditional on the panel's votes.

```{r}
library(dplyr)
library(tidyr)
library(tibble)
library(gt)

# Priors to compare
priors <- tribble(
  ~Beta,                ~alpha, ~beta,
  "Beta(1.0, 1.0)",          1,      1,
  "Beta(0.3, 2.7)",      0.3,    2.7,
  "Beta(0.05, 0.45)",    0.05,   0.45
)

# Panel outcomes (k = # voting "incorrect" out of n = 5)
panels <- tribble(
  ~Panel, ~k,
  "5-0",   5,
  "4-1",   4,
  "3-2",   3
) |>
  mutate(Panel = factor(Panel, levels = c("5-0", "4-1", "3-2")))

n <- 5

# Posterior mean E[q | k] = (alpha + k) / (alpha + beta + n)
results <- priors |>
  crossing(panels) |>
  mutate(
    posterior_mean = (alpha + k) / (alpha + beta + n)
  ) |>
  select(Beta, Panel, posterior_mean) |>
  pivot_wider(names_from = Panel, values_from = posterior_mean)

# Table
results |>
  gt(rowname_col = "Beta") |>
  fmt_number(columns = everything(), decimals = 2) |>
  cols_align(align = "center", columns = everything())
```
<p style="font-size:0.9em; color:#666; margin-top:-0.5em; text-align:center;">
  <em>Mean posterior probability of decision being incorrect, conditional on observed panel decision.</em>
</p>

So, depending on your prior, the maximum estimate of the probability of an incorrect decision based on the panel's vote would be 92% for 5-0, 74% for 4-1, and 57% for 3-2: not far off a coin flip!

This sort of thing highlights a wider problem with statistics which is that there are different ways to spin the exact same result. The BBC have gone with "VAR mistakes are on the rise!" whereas I look at the panel's voting splits and think this just shows you how difficult a referee's job is! Even a panel of experts, with all the time in the world, cannot agree what is the correct decision.

So what is a better approach? At minimum, I think they should require at least a 4-1 vote for a decision to be declared incorrect, and preferably 5-0. This is in line with the "clear and obvious error" spirit that everyone seems to be in favour of, until there is a decision they don't like.

A similar system exists in a jury, where all 12 members have to agree to reach a guilty verdict. Perhaps we should have a VAR jury? Five different refs sat in different rooms, and a 4-1 majority required to overturn the on-field decision.

I don't hugely care about VAR's accuracy, to be honest. There will be incremental improvements, but it feels like that will just move us to a new frontier of what everyone will complain about; we can already look forward to [VAR reviews of corners](https://www.bbc.com/sport/football/articles/c62lgler7rlo).

Those who anticipated VAR ushering in a new era without weekly officiating controversies were sadly mistaken. We did lose the spontaneity of goals being scored though, and now it's "ok maybe we've scored, but I'll have to wait for the replay to see if that winger was half a yard offside for that pass in the buildup". Was it worth it?
