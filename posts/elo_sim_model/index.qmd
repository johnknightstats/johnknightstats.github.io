---
title: "Building an Elo Simulation Model for Retrospective League Winner Daily Probabilities"
draft: true
image: viz/cond_plots.png
author: "John Knight"
date: 2025-07-11
date-format: "D MMMM YYYY"
description: "Here I show how the model was built"
categories: ["football", "statistics"]
format:
  html:
    title-block-style: none
    css: ../styles.css
execute: 
  echo: false
  eval: true
engine: knitr
editor: source

---

*Here I show how the model was built*

---

I hope you enjoyed my recent article looking at the worst league title collapses in English football history. If not, you can read it [here](https://johnknightstats.com/posts/title_collapses/).

Now I will go into a little more depth on the model I used to simulate scores from Elo ratings. There's a bit of mathematics involved, but nothing too heavy.

As mentioned in the original article, daily Elo Ratings were taken from [Club Elo](http://clubelo.com/). FBRef has all results for the [entire history](https://fbref.com/en/comps/9/history/Premier-League-Seasons) of the English top flight, which is pretty cool. I had to create some mappings to link the FBRef team names to the Club Elo ones. You can find all my code and data files for this project in my [GitHub repo](https://github.com/johnknightstats/elo-league-sims).

Since the Elo ratings only begin in 1940, we will have to ignore results prior to World War 2. Our first task, as always, is to do some data inspection.

I took the decision not to account for varying goal rates in this model, because it adds a fair amount of complexity and I didn't think it would add a huge amount given the task at hand. Nonetheless, it is still interesting to see how the number of goals per game has changed over the last 80 years, especially the huge decrease in the latter half of the 1960s.

::: {.centered-block .max-70}
<img src="viz/goals_pg_seasons.png"/>
:::

We can also take a look at home supremacy in the English top flight through the years. This has shown a steady decrease, presumably due to improved travel and accommodation for away sides. Not many teams traveled with their own chef in the 1950s.

And of course, the 2021 value close to zero helped to answer the question "how much of home advantage is due to the fans?" Answer: a lot.

::: {.centered-block .max-70}
<img src="viz/home_suprem_seasons.png"/>
:::

Finally, we can summarise the mean Elo rating of English top flight teams over the same time period. The overall trend is upward, and it is also notable that the standard deviation (shown by the light blue band) gets wider as time goes on. The spread of team quality, as I'm sure you are aware, is much wider than it used to be.

::: {.centered-block .max-70}
<img src="viz/elo_mean_seasons.png"/>
:::

Now we can look at the Elo rating system itself. It was originally designed by Arpad Elo to rank chess players, but can be extended to any zero-sum game with two players or teams. When two teams play against one another, they are effectively gambling a portion of their points, with the better team (according to their Elo ratings) putting more of their points at stake than the inferior team. Since the winner gains the same number of points as the loser loses (i.e. 'zero-sum'), the population of ratings sits around a consistent long-term average - typically 1500, but any arbitrary number can be chosen.

You can find a full explanation of how Club Elo calculates its ratings [here](http://clubelo.com/System). There are adjustments for margin of victory, home advantage, and what they call "tilt" (whether a team's games are higher or lower scoring than average).

We can use Club Elo's formula to calculate the forecast probability for each game in the data, with 0.5 awarded for a draw. Below I have plotted the deciles of teams' predicted probability to see how they match up with actual results. 

::: {.centered-block .max-70}
<img src="viz/elo_deciles.png"/>
:::

The plotted points should approximately follow the dashed line but as you can see, Elo seems to overestimate favourites' chances and underestimate underdogs' chances. To reiterate, this is when allotting 0.5 points for a draw. I don't know how it would fare when trying to predict actual win-draw-lose results instead, or if it's possible to derive that simply from the Elo difference. 

But that is our next task!

Clearly if we are going to simulate league tables, we need actual scores rather than just results. And we certainly need some draws and not "half wins". So how can we simulate realistic football scores?

Now let me make it clear at this point, I am not breaking any new ground here. There is a large body of work dedicated to the distribution of football scores going back as far as the 1950s. For further reading I recommend Reep, Maher, Dixon-Coles. But I find it useful and educational to try and build these things yourself step by step, so here we go. I promise to try and keep the maths (math for American readers) as light as possible.

In statistics, when we have an event that will happen some random number of times during a time interval, it is common to use the Poisson distribution. Given that we know the average number of goals scored by teams in our dataset (1.42), we can simply plug that into R's dpois function and it will give us the probability of 0 goals, 1 goal, 2 goals, and so forth. Then we can compare these probabilities with the actual observed relative frequency of each score:

::: {.centered-block .max-70}
<img src="viz/poisson_v_obs.png"/>
:::

So what's going on? Poisson is overpredicting scores of 1, 2 and 3, but underpredicting 0, 4, 5, and all higher scores up to infinity. This is a problem known as overdispersion. The Poisson distribution relies on an assumption that the mean is equal to the variance. In reality, this assumption is rarely satisfied. Indeed, the variance of our football scores is 1.65 compared to the mean of 1.42. That's why the actual scores are more spread out (more 0s, more 5s and 6s) compared to the Poisson predictions (more 1s and 2s).

This is where the negative binomial distribution comes to our rescue! The negative binomial distribution in its most common form would be used to model things like "if I have a 70% chance of making each free throw, what is the probability that I miss exactly 5 free throws before I make my 10th successful free throw?" Anyone who took an undergraduate probability class will be familiar with fun questions like that.

But the negative binomial distribution can be expressed in a different way to make it very similar to the Poisson distribution so that it has a mean but also an additional parameter *\theta* which is known (amongst other things) as the dispersion parameter. This allows us to build a Poisson-like distribution but to toggle the shape by changing *\theta*. The two distributions are related - in fact, if you keep increasing *\theta*, the negative binomial gets closer and closer to Poisson.

Below I have added the negative binomial distribution (with *\theta* = 10) alongside the Poisson and the actual observed results:

::: {.centered-block .max-70}
<img src="viz/poisson_v_nb_v_obs.png"/>
:::

Not bad! We instantly see that this is a huge improvement over Poisson, and honestly does a pretty good job on every scoreline. To demonstrate how increasing *\theta* affects the negative binomial distribution, here is the same plot with *\theta* = 10,000. It is now indistinguishable from the Poisson distribution:

::: {.centered-block .max-70}
<img src="viz/poisson_v_nb_v_obs_10k.png"/>
:::

So we're done, right? Just use a negative binomial distribution with *\theta* = 10 to model football scores. Not so simple, I'm afraid! That would be making an assumption that you can generate a score for team A, and then generate a score for team B, as if the two things happen independently. But is that a fair assumption? 

To test that assumption we need to look at the *conditional* distribution of a team once we know how many goals their opponents have scored:

::: {.centered-block .max-70}
<img src="viz/cond_plots.png"/>
:::

So now we can see there is a problem. When the opponent has scored 0, the distribution underestimates 0. When the opponent has scored 1, the distribution underestimates 1. When the opponent has scored 2, the distribution underestimates 2, and so on.

So in actual football, there are more draws than would be expected by two independent scores. This is not hard to explain: when a game is level, teams will take relatively fewer risks. When a team is trailing by one goal, they will throw men forward and make the game more open in pursuit of an equaliser. So the overall effect is that results are slightly more likely to settle on draws.

We also have another problem that we haven't addressed yet. We are generating scores for an aggregate of all teams in our dataset. But teams' abilities can vary significantly! So we need an algorithm that gives us the mean and distribution for one team, based on the difference in the teams' Elo ratings, and then the mean and distribution for the second team, based not only on the Elo ratings but also on the score of the first team!

The simplest way to approach this is by first generating the score of the favourite (i.e. the team with the higher Elo rating after home advantage has been applied). I fit this with a simple quadratic model of the difference in Elo ratings. Below we can see how this compares with the negative binomial with *\theta* = 35. Remember, this is now an aggregation of different predictions across all of the games in the dataset.

::: {.centered-block .max-70}
<img src="viz/fav_goals_fitted.png"/>
:::

Pretty good! So the favourites at least seem to be sorted, and we can see that the negative binomial with *\theta* = 35 is superior to Poisson. But what about the underdog? It's slightly more complex to fit the underdog's mean goals scored because now we have to account for the number of goals scored by the favourite. In addition, I added interaction terms to the quadratic model. A simple way to explain this is, if Real Madrid were playing against Tranmere Rovers and I told you Real Madrid scored 3 goals, would it change your prediction of how many Tranmere scored? Probably not much. If I told you Tranmere scored 3? Clearly your estimated for Real Madrid scoring 3+ should increase. So that is an interaction between two variables.

So let's see how the underdog scores come out, conditional on the goals scored by the favourite:

::: {.centered-block .max-70}
<img src="viz/underdog_cond_plots_dispersion_5.png"/>
:::

So clearly there is a problem. I tried a few different values of *\theta* - the above plots are with *\theta* = 5 because that's a good fit when the favourite scores 0. But it's a poor fit for the other scores. So what we probably need is a different *\theta* depending on the favourite's score!

I trained a function to find the optimal *\theta* for each value of the favourite's score, and these were the results:

Fav Goals | Theta

0 | 8.4
1 | 35.3
2 | 61.3
3 | 10000

Let's see how the conditional plots look with these fitted dispersions:

::: {.centered-block .max-70}
<img src="viz/underdog_cond_plots_fitted_dispersion.png"/>
:::

As Roy Walker would say: "It's good, but it's not the one." So now it's time to use some brute force. We know that the underdog's score is more likely to match the favourite's score, so let's do exactly that. I repeated the last optimization, but this time with an extra parameter that inflates the probability of the same score as the favourite. Here are the results:

0 | 569  | 0.0935
1 | 14.1 | 0.0496
2 | 321  | 0.00894
3 | 198  | 0.00588

So when the favourite scores 0, there is a big boost to the probability of the underdog scoring 0. When the favourite scores 1, the effect reduces by almost half. For 2 or more goals, the inflation is minimal.

Now how does that look in the conditional distribution?

::: {.centered-block .max-70}
<img src="viz/underdog_cond_pointmass.png"/>
:::

I'm pretty happy with that overall. The scores seem to track pretty well, so we can now generate a probability for every scoreline (0-0, 2-1, 3-3, etc) for every game in the dataset and derive probabilities of win-draw-loss for each team. Let's calibrate our model by comparing these probabilties with the actual results and scores:

::: {.centered-block .max-70}
<img src="viz/deciles_new.png"/>
:::

::: {.centered-block .max-70}
<img src="viz/wdl.png"/>
:::

::: {.centered-block .max-70}
<img src="viz/model_v_scores.png"/>
:::

The deciles track pretty nicely with our dashed line falling inside all the error bars except one. Note that the probabilities are different to the earlier decile plot because that one was using "half wins" when a game was drawn, whereas this plot assumes a draw is not a win (i.e. 0, the same as a loss).

The W-D-L plot and correct scores plot are close...there just seems to be a stubborn difference in the 1-1 predictions for some reason. But for the task at hand, this is easily adequate.

What we have found in this process is somewhat similar to the Dixon-Coles model which uses a Poisson distribution with inflated probabilities of low scores (0-0, 0-1, 1-1, 1-0). Please check out their work as it will be far more mathematically rigorous than anything I have done here.

Thanks for reading, and as I mentioned earlier, you can find all the code relating to this project at my [GitHub repo](https://github.com/johnknightstats/elo-league-sims).

{{< include ../../includes/_article_footer.qmd >}}

© 2025 John Knight. All rights reserved.
