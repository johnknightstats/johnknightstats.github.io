---
title: "Variance in betting results"
author: "John Knight"
draft: true
format:
  html:
    title-block-categories: true
    css: ../../../styles.css
categories: ["betting", "statistics"]
---

We've all been there. You develop a new betting system, you get off to a hot start and think you have found the Golden Goose - but then the wins dry up, and you end up back at square one.

One of the trickiest parts of gambling is assessing whether or not you have an edge. While it can be easy to get overexcited after some positive early results, there is also a danger of giving up on a system too early because you had some bad luck early on.

For any serious bettor, it's important to keep thorough records of all your bets, and an essential part of that process is to record your estimate of the *true* price for every bet. This enables you to calculate your expected profit after *n* bets, and assess how your actual results are performing relative to expectations.

I created a simple [tool](https://johnknightstats.com/tools/betting-variance-tool.html) that compares two distribution curves after *n* bets: one if the market price is correct, and the other if *your* price is correct.

For example, let's say you are placing $100 bets at decimal odds of 2.0, when you believe the *true* price is 1.8. This means you think there is a 55.6% chance of the bet winning when the market says it is 50%, which is a very solid edge if you can find it consistently.

After 100 bets, the curves look like this:

::: {.centered-block}
<img src="viz/bets_100.png"/>
:::

Now, 100 bets is a lot! Especially if it's a system that only throws up a handful of bets a week. This could be several months' work - and yet clearly there is a lot of cross over between the two curves. You could quite easily be losing with a very good system, OR you could be showing a decent profit from pure guesses.

If we increase the number of bets to 1000, the two curves are now a lot more distinct, and the probability of an overall loss IF your 1.8 price is correct is virtually nil.

::: {.centered-block}
<img src="viz/bets_1000.png"/>
:::

So this is all great in theory, but what about in practice? Not all bets are around even money; would the distribution really follow a nice smooth bell curve?

The answer is: basically, yes! Thanks to the central limit theorem, the mean of a sample will approximate a normal distribution as long as the sample is sufficiently large. And what is sufficiently large? The rule of thumb for statisticans is 30, based on empirical observation. Let's see how that looks.

I created a small function in R that calculates the profit or loss after 30 bets at 2.0 (even money). Then, I wrote a second function that simulated the 30 bets 10,000 times and plotted a histogram of the outcomes.

::: {.centered-block}
<img src="viz/mean_distribution_2.png"/>
:::

As promised, after only 30 bets we can see that the distribution of the mean outcome resembles a beautiful bell shape.

But that's for the dream scenario where we are betting at even money. What about a more skewed distribution? I changed the odds to 5.0 - perhaps you are betting on away underdogs in football matches. Here is how the distribution looks after 30 bets:

::: {.centered-block}
<img src="viz/mean_distribution_5.png"/>
:::

So you can see that even with odds of 5.0, after 30 bets the overall distribution is fairly close to normal - certainly close enough to be robust to statistical tests.

If we move to more extreme skewed distributions, that's when the sample size of 30 becomes too small. Here I demonstrate 30 bets at odds of 20.0 - perhaps you are betting on horses, or golfers to win a tournament:

::: {.centered-block}
<img src="viz/mean_distribution_20.png"/>
:::

The distribution is now skewed to the right. Clearly your maximum loss is capped at 30 units, but wins of greater than 30 are quite feasible if 3 or 4 winners come in. So it would not be appropriate to use a normal distribution to represent these 30 bets. But if we increase the number of bets to 100 at the same odds, then we start to see the normal distribution shape emerge.

::: {.centered-block}
<img src="viz/mean_distribution_20a.png"/>
:::

So generally speaking, it should be fine to use a normal distribution to represent the range of outcomes one might reasonably expect from a series of independent bets, as long as you have a sample of around 30 bets or more.

## A real life example

So far, I have just been giving hypothetical examples where the odds are the same for every bet. You could actually use the binomial distribution if that were the case, but in reality you are going to have lots of different prices and stake sizes.

Because each bet is an independent Bernoulli event, it is simple to calculate the variance for each bet. And a great property of variance is that the variance of a sum of variables is simply the sum of the variance of each variable!

I made this simple Excel file with formulas for calculating total variance.

*** Insert Excel file here ***

Now here's a real life example from a betting system I was recently testing for small stakes. This is something I strongly advise any time you think you have found something: test it with real money! Back-testing or paper trading is one thing, but there is no substitute for actually getting real money down.

*** Show real example here ***

::: {.table}
<table class="fancy-table">
  <tr><td>Total number of bets</td><td>405</td></tr>
  <tr><td>Total staked</td><td>£29,495.07</td></tr>
  <tr><td>Total profit</td><td>£2,824.24</td></tr>
  <tr><td>ROI</td><td>9.58%</td></tr>
</table>
:::


::: {.centered-block}
<img src="viz/bet_record_plot_405.png"/>
:::

::: {.table}
<table class="fancy-table">
  <tr><td>Total number of bets</td><td>830</td></tr>
  <tr><td>Total staked</td><td>£61,279.54</td></tr>
  <tr><td>Total profit</td><td>-£413.36</td></tr>
  <tr><td>ROI</td><td>-0.67%</td></tr>
</table>
:::


::: {.centered-block}
<img src="viz/bet_record_plot_830.png"/>
:::

*** Bayesian approach ***

*** Variance reduction methods ***